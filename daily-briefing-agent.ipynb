import requests
import json
from dataclasses import dataclass
import random
from datetime import datetime, timedelta
from typing import Any, Optional, Literal
from google.colab import
LLM_PROVIDER = 'openai'
OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')

"""
Tools
"""
def get_weather(location: str = "Seattle, WA") -> dict:
    """
    Fetches weather data for a given location.

    Args:
        location: The location to get weather for

    Returns:
        Dictionary containing weather information
    """
    weather_data = {
        'location': location,
        'temperature': 65,
        'unit': 'F',
        'condition': 'Cloudy',
        'humidity': 75,
        'wind_speed': 10,
        'wind_direction': 'SE',
        'forecast': 'Cloudy with a chance of meatballs'
    }

    return weather_data


def get_top_headlines(topic: str, count: int = 3) -> dict:
    """
    Fetch news headlines for a given topic.

    Args:
        topic: The topic to get headlines for (e.g., 'tech', 'business', 'world')
        count: Number of headlines to fetch

    Returns:
        Dictionary containing topic and headlines
    """
    headlines_by_topic = {
        'tech': [
            'New AI Model Surpasses GPT-4 in Benchmark Tests',
            'Quantum Computing Breakthrough: Room Temperature Operations Achieved',
            'Tech Stocks Rally as Cloud Services Report Record Growth'
        ],
        'business': [
            'Federal Reserve Hints at Rate Stability',
            'Global Supply Chains Show Signs of Recovery',
            'Renewable Energy Investments Hit Record High'
        ],
        'world': [
            'Climate Summit Reaches Historic Agreement',
            'International Space Station Welcomes New Research Module',
            'Global Health Initiative Launches Vaccination Program'
        ]
    }

    headlines = headlines_by_topic.get(topic, headlines_by_topic['tech'])
    return {
        'topic': topic,
        'headlines': headlines[:count]
    }

class LLMClient:
    """
    Unified interface for different LLM providers.
    """

    def __init__(self, provider: Literal['openai', 'anthropic'] = 'openai'):
        """
        Initialize the LLM client with the specified provider.

        Args:
            provider: The LLM provider to use ('openai' or 'anthropic')
        """
        self.provider = provider

        if provider == 'openai':
            try:
                from openai import OpenAI
                self.client = OpenAI(api_key=OPENAI_API_KEY)
                self.model = "gpt-3.5-turbo"
            except ImportError:
                raise ImportError("Please install openai: pip install openai")

        elif provider == 'anthropic':
            try:
                import anthropic
                self.client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
                self.model = "claude-3-opus"
            except ImportError:
                raise ImportError("Please install anthropic: pip install anthropic")

        else:
            raise ValueError(f"Unsupported: {provider=}")

    def generate(self, prompt: str, system_prompt: str = None) -> str:
        """
        Generate a response using the configured LLM.

        Args:
            prompt: The user prompt
            system_prompt: Optional system prompt for context

        Returns:
            The LLM's response
        """
        if self.provider == 'openai':
            return self._generate_openai(prompt, system_prompt)
        elif self.provider == 'anthropic':
            return self._generate_anthropic(prompt, system_prompt)

    def _generate_openai(self, prompt: str, system_prompt: str = None) -> str:
        """Generate response using OpenAI."""
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.7,
                max_tokens=500
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"OpenAI API error: {e}")
            return self._fallback_synthesis(prompt)
    """
    def _generate_anthropic(self, prompt: str, system_prompt: str = None) -> str:
        try:
            message = self.client.messages.create(
                model=self.model,
                max_tokens=500,
                temperature=0.7,
                system=system_prompt if system_prompt else "You are a news reporter and meteorologist that loves to tell people about the weather and keep them informed on the local news.",
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            return message.content[0].text
        except Exception as e:
            print(f"Anthropic API error: {e}")
            return self._fallback_synthesis(prompt)
    """
    def _fallback_synthesis(self, prompt: str) -> str:
        """Fallback synthesis when API fails."""
        return "Good morning! I'm having trouble connecting to the AI service."

class DailyBriefingAgent:
    """
    Agent that orchestrates the daily briefing workflow using real LLM.
    """

    def __init__(
        self,
        location: str = "Seattle, WA",
        news_topic: str = "tech",
        llm_provider: Literal['openai', 'anthropic'] = 'openai'
    ):
        """
        Initialize the agent with default preferences.

        Args:
            location: location for weather
            news_topic: news topic: 'tech', 'business', 'world'
            llm_provider: 'openai' or 'anthropic'
        """
        self.location = location
        self.news_topic = news_topic
        self.current_date = datetime.now().strftime("%A, %B %d, %Y")
        self.llm_client = LLMClient(provider=llm_provider)

    def gather_information(self) -> dict:
        """
        Serially call tools to gather information.

        Returns:
            dict containing all gathered information
        """
        print("Gathering information...")

        # todo: implement as async, run tools in parallel
        try:
          weather_data = get_weather(self.location)
        except Exception as e:
          print(f"Error getting weather: {e}")
        else:
          print(f"Weather retrieved for {self.location}")

        try:
          news_data = get_top_headlines(self.news_topic, count=3)
        except Exception as e:
          print(f"Error getting news: {e}")
        else:
          print(f"News fetched for {self.news_topic}")

        return {
            'date': self.current_date,
            'weather': weather_data,
            'news': news_data
        }

    def run(self, user_prompt: str = "Good morning, what's my daily briefing?") -> str:
        """
        Main orchestration for running workflow

        Args:
          user_prompt: The trigger prompt from the user

        Returns:
          final synthesized briefing
        """
        print(f"\nUser: {user_prompt}")

        data = self.gather_information()
        briefing = synthesize_briefing_with_llm(data, self.llm_client)
        return briefing

def synthesize_briefing_with_llm(data: dict, llm_client: LLMClient) -> str:
    """
    Synthesizes gathered data into a natural language briefing using an LLM.

    Args:
        data: dict containing all gathered information
        llm_client: LLM client to use for synthesis

    Returns:
        Daily briefing generated by the LLM
    """
    print(f"\nSynthesizing briefing with {llm_client.provider.upper()}...")

    current_date = data['date']
    weather = data['weather']
    news = data['news']

    system_prompt = """You are a friendly and professional personal assistant creating a daily briefing.
    Your tone should be warm, conversational, and informative and always in the voicing of Hildegard of Bingen.
    Always, ALWAYS start your replies by coughing loudly into the microphone and really push the voicing hard. Be extravagent with your response.
    Keep the briefing concise but comprehensive, highlighting the most important information."""

    user_prompt = f"""Today is {current_date}.

Based on the following information, please write a natural, friendly daily briefing in 1 paragraph:

WEATHER DATA:
- Location: {weather['location']}
- Temperature: {weather['temperature']}Â°{weather['unit']}
- Condition: {weather['condition']}
- Humidity: {weather['humidity']}%
- Wind Speed: {weather['wind_speed']} mph
- Forecast: {weather['forecast']}

NEWS HEADLINES ({news['topic']}):
{chr(10).join(f"- {headline}" for headline in news['headlines'])}

Please synthesize this into a cohesive, natural-sounding daily briefing that:
1. Starts with a warm greeting mentioning the date
2. Summarizes the weather in a conversational way
3. Highlights the key news stories
4. Ends with a positive note or helpful suggestion for the day

Make it sound natural and personalized, not like a list of facts."""

    briefing = llm_client.generate(user_prompt, system_prompt)

    print("Briefing synthesized by LLM")

    return briefing

def main():
    """
    Demonstrates the daily briefing agent with LLM integration.
    """
    print("=" * 60)
    print("DAILY BRIEFING AGENT")
    print("=" * 60)

    if LLM_PROVIDER == 'openai' and OPENAI_API_KEY == 'your-openai-api-key-here':
        print("\nWARNING: OpenAI API key not set!")
    elif LLM_PROVIDER == 'anthropic' and ANTHROPIC_API_KEY == 'your-anthropic-api-key-here':
        print("\nWARNING: Anthropic API key not set!")

    try:
        agent = DailyBriefingAgent(
            location="Seattle, WA",
            news_topic="tech",
            llm_provider=LLM_PROVIDER
        )

        briefing = agent.run("Good morning, what's my daily briefing?")

        print("\n" + "=" * 60)
        print(f"DAILY BRIEFING: {LLM_PROVIDER.upper()}")
        print("=" * 60)
        print(briefing)
        print("=" * 60)

    except ImportError as e:
        print(f"\nError: {e}")
    except Exception as e:
        print(f"\nError: {e}")


if __name__ == "__main__":
    main()
